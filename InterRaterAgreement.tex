\Medicine{Inter-rater agreement}{A common method for measuring the
  level of agreement is an {\em inter-rater agreement studies}. In
  such studies several doctors are provided with the same patient
  file and are asked to give a diagnosis.

  A common measure of of the agreement between two raters is the Cohen's kappa
  coefficient, usually denoted by $\kappa$. \yoav{How is the kappa
    coefficient generalized to studies where there are more than two
    raters ?} Kappa is computed from two quantities: $0\leq a\leq 1$
  is the fraction of patient files on which the two raters agree, 
  and $0\leq c\leq 1$ is the fraction of agreements that would occur
  by chance. The definition of kappa is $\kappa=\frac{a-c}{1-c}$.

  If $\kappa=1$ The raters always agree, if $\kappa=0$ the rate of
  agreement corresponds to chance, and if $\kappa<0$ then the rate of
  agreement is lower than chance, i.e. the two raters tend to have
  different opinion. An
    interpretation of $\kappa$ recommended by Cohen
    \cite{mchugh2012interrater} is: $\kappa\leq 0$: no agreement, $0< \kappa\leq 0.20$:none to slight
    agreement, $0.2<\kappa\leq 0.40$: fair agreement,
    $0.4<\kappa\leq 0.60$ as moderate agreement, $0.6<\kappa\leq
    0.80$: substantial agreement, and $0.8<\kappa\leq 1.00$: perfect
    agreement.

    \yoav{can we give a list of kappa values for some common
      diagnostics here?}
  }
