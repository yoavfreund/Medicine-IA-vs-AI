\Medicine{Inter-rater agreement}{A common method for measuring the
  level of agreement is an {\em inter-rater agreement studies}. In
  such studies several doctors are provided with the same patient
  file and are asked to give a diagnosis.

  A common measure of of the agreement between two raters is the Cohen's kappa
  coefficient, usually denoted by $\kappa$.  Kappa is computed from two more basic quantities: $0\leq a\leq 1$
  is the fraction of patient files on which the two raters agree, 
  and $0\leq c\leq 1$ is the fraction of agreements that would occur
  by chance. The definition of kappa is $\kappa=\frac{a-c}{1-c}$.

  If $\kappa=1$ The raters always agree, if $\kappa=0$ the rate of
  agreement corresponds to chance, and if $\kappa<0$ then the rate of
  agreement is lower than chance, i.e. the two raters tend to have
  different opinion. An
    interpretation of $\kappa$ recommended by Cohen
    \cite{mchugh2012interrater} is: $\kappa\leq 0$: no agreement, $0< \kappa\leq 0.20$:none to slight
    agreement, $0.2<\kappa\leq 0.40$: fair agreement,
    $0.4<\kappa\leq 0.60$ as moderate agreement, $0.6<\kappa\leq
    0.80$: substantial agreement, and $0.8<\kappa\leq 1.00$: perfect
    agreement.

    For example, in the sleep stage annotation, the pairwise Cohen's kappa over 5 sleep experts (totally 10 pairs) is on average 65\% over normal subjects and about 59\% over subjects with sleep apnea \cite{norman2000interobserver}. In other words, the inter-rater agreement is substantial over normal subjects and moderate over subjects with sleep apnea.
    %\yoav{can we give a list of kappa values for some common      diagnostics here?}
  }
