\documentclass[11pt]{pnas-new}
% \documentclass[10pt]{article}
\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{xcolor,ulem}
\usepackage{mdframed,ulem}
\usepackage{wrapfig}
\usepackage{multicol}
\setlength{\columnsep}{1cm}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{lipsum}
\newlength{\strutheight}
%\usepackage{soul} % for strike-through (\st)

\author[1]{Hau-Tieng Wu}
\author[2]{Yoav Freund}

\affil[1]{Duke, Mathematics and Statistical Science, Durham, 27708, USA}
\affil[2]{UCSD, Computer Science, San Diego, 92093, United States. yfreund@eng.ucsd.edu}


\title{Trusted digital doctors know their limits}

\input{macros}
\renewcommand{\input}[1]{}

\begin{abstract}

  The meteoric rise of AI in general and Deep Learning in particular
  is generating great excitement throughout academia and commerce, and
  in particular in medicine\cite{topol2019deep,
    wachter2015digital}. With some some high-profile claims
  that AI will soon replace humans in many medical specialties.

  In this position paper we present an alternative view. We contrast
  {\em Artificial Intelligence} with {\em Intelligence Augmentation}
  and argue that the second is more likely to benefit the patient than
  the first. We provide evidence to this argument and present a vision
  in which easier decisions are delegated to computers, while the more
  difficult ones are handled by humans.

\end{abstract}

\begin{document}
\settoheight{\strutheight}{\strut}

 
\maketitle

%\thispagestyle{firststyle}
\iffalse
\section{issues to be resolved}

\hautieng{As we discussed, I followed the medical journal convention and changed the author order so that you are the senior/corresponding author.}

\begin{itemize}
    \item Introduce the term "Healthcare Provider" with acronym HP, and use throughout instead of doctors nurses etc. \hautieng{I am not sure what is the best way to do it. In many places, "doctor" are clearly more precise than HP. So I leave this part to you.}
    \item Patient compliance - how can IA help make sure that patients take their meds, don’t drink excessively etc. \hautieng{done}
    \item Compensation: Doctors that generate quality data should own this data, distribution mechanisms should compensate the doctor for his/her contribution. \hautieng{As we discussed, this might be a bit off the topic. This is more like a regulatory issue. Let's discuss it if you have a different viewpoint.}
    \item A few grammatical corrections:
    -insert 2 page 2 last line “piopsies” instead of biopsies
    -last line page 2 spelling: “therefor” (needs an e) therefore 
    -insert page 7 we quote from Robert Rechter’s book... (53) Reference @ 53 indicates “Robert Wachter” as author. \hautieng{I am not fully sure what you want...}

\end{itemize}

To Highlight a change use this macro: \change{old text}{new text}
\fi

\section{Introduction}

The meteoric rise of  Artificial Intelligence (AI) and Deep learning raises the possibility that
doctors will be replaced by computers~\cite{Mukherjee2017}. Geoff Hinton,
a famous deep learning researcher said in 2017: ``It's just completely
obvious that in ten years deep learning is going to do better than
Radiologists ... They should stop training radiologists now''.

The predictions of Sebastian
Thrun~\cite{Mukherjee2017,esteva2017dermatologist}, another leader in
machine learning, are less disruptive: ``... deep learning devices
will not replace dermatologists and radiologists. They will {\em
  augment} professionals, offering the expertise and assistance''. 
  The term "Augmented Intelligence" appears frequently in recent 
  policy reports from medical associations~\cite{american2019augmented,AAD2019augmented}. Collaborations between computers and human care-givers are seen as a desirable goal.
  In this article we suggest how such a collaboration between human and computer might unfold in practice. Central to our approach is to allow the computer to quantify it's own uncertainty and, when appropriate, output "I don't know" (which we will abbreviate as IDK).
 
\input{AIIABox} The argument as to whether human healthcare providers (HP)~\footnote{Our discussion applies to all healthcare providers, including Doctors, Nurse Practitioners and Nurses. To simplify the references to this group of professionals we use the acronym HP, which stands for "Healthcare Provider"}  will be replaced or empowered by computers is part of a wider debate between AI, whose goal is to imitate human behaviour  and Intelligence Amplification (IA), whose goal is to extend human capabilities. Computers with AI are sometimes called "AI agents", we contrast computers with IA by referring to them as "IA Helpers" (IAH). Helpers are expected to help when they can, and say IDK when they cannot. In any case, they are not allowed to take actions regarding the patient.
Of the many potential uses of AI/IA in medicine, we focus on diagnostics.
AI-driven diagnosis are expected to become a reality much sooner than other medical activities such as surgery~\cite{topol2019deep}. 

Central to our argument is a quantification of {\em prediction
  confidence}. Such quantification is needed to avoid premature
diagnostic conclusions, and to decide which additional tests or
consultations might be needed. Consider a doctor that is asked
to diagnose a patient with complex or conflicting symptoms. A careful
doctor will admit their uncertainty and perform additional tests or
ask a specialist. A less careful, overly self confident doctor is
more likely to give an incorrect diagnosis and choose an ineffective or even damaging
treatment plan.

An AI agent, expected to be better than the human HP, is more likely to 
end up behaving like an overly confident doctor. An IAH, aware of
its own limitations, will give advice only when the evidence is
strong and otherwise say IDK.

In the following we explore these ideas. We
start with a critique of one of the papers that claims that AI agents
can outperform human diagnosticians.

\section{Black-box learning}
\label{sec:ground-truth}

Deep learning (DNN) is a powerful black-box learning algorithm. Examples of applications of DNN to medical diagnosis include arrhythmia classification 
from single channel electrocardiogram \cite{hannun2019cardiologist}, diabetic retinopathy detection from retinal fundus photographs \cite{gulshan2016development}, skin cancer diagnostics~\cite{esteva2017dermatologist} as well as many others.

\input{Supervised}
%The data for supervised learning consists of a large collection of
%(input, output) pairs. For medical diagnosis, usually the input is medical
%information for the patient (Heart rate, blood tests, X-ray images
%etc.) and the output is the diagnosis. This output is output is assumed to be the undisputed
%``ground-truth''. Unfortunately, this assumption is problematic in the context of medical diagnostics. 
%It is not uncommon that different diagnosticians give different diagnoses for the same medical %information which 
%makes it hard to assign a ground to each instance. We will return to this important issue in the next %section.

%The other important assumption made in supervised learning is that the
%generated classifier is tested using the same distribution of examples
%as that of the training set. In practice, however, this is often hard to achieve.

\input{SkinCancer}
In a highly cited paper in the journal Science~\cite{esteva2017dermatologist} the authors 
makes the claim that DNNs can perform diagnostics as well as, or better, than board certified dermatologists. 

A fundamental problem with the experiments presented in the paper is that the test data used does not represent 
a realistic deployment of the system.
The data used in the experiment was {\em retrospective},
i.e., it was collected from the records of past patients for which both
a skin image and a biopsy were available. Normally, patients get
biopsied only if the dermatologist thinks there is a significant
chance of {\bf malignancy}. As a result, a retrospective study that is
based on patients for whom a biopsy was taken is likely to
over-represent malignant patients and therefore be biased. If an image-based classifier
is trained on the biased data, its performance on unbiased test data
is likely to be worse. Specifically, when the classifier is applied to skin
images of un-diagnosed patients it is likely to over-diagnose them as
malignant, potentially resulting in an increase of the number of unnecessary biopsies.

\section{Uncertainty in medicine}

In medical diagnostics the {\em ground truth} are often unavailable, and uncertainty in diagnosis is common. 

{\color{blue}One way to confirm a diagnosis, or obtain the ground truth, is to collect more information through additional examinations and tests. However, cases (patients) differ in their complexity, and the attending medic might judge additional tests to be unnecessary, in which case the outcome of these tests will not appear in the observational study. 
In {\em simple} cases, the initial exam is enough for the doctor to confidently choose a treatment. In a more complex case, the doctor might ask for multiple tests and visits, refer the patient to a specialist, consult colleagues, journals and books, etc, to narrow the set of likely diagnostics and choose a treatment plan. 
%
In addition, diagnosis is not a simple input-output mapping. Rather, it is an iterative process which reduces uncertainty over time. To illustrate this, consider the diagnostics of a patient that is treated in a clinic.  When a patient arrives at the clinic for the first time, all diagnostics are possible. After a physical exam and an interview with a doctor, many possibilities are eliminated.
%
Another way is following up on the treatment plan. However, correct diagnosis is only one of many factors that influence the long term health of the patient. Others include choice of treatment, medication adherence, changes in living and working environments, diet, stress, etc.





Given the difficulties in measuring an undisputed "ground truth" for diagnostics, we suggest that the goal of learning be set not to predicting a {single} "true" diagnostic, but rather to {taking} the actual diagnostics given by the HP as the ground truth. In other words, we change the goal from performing better than the human to that of performing almost as well as the humans. Which means that in order for IA to aid the HP in the diagnostic process, uncertainty or disagreement among the HP should be reflected as uncertainty in the output of the IAH, and 
 it has to have a way to express both what is knows and what it {\em does not} know.
%
This conforms to the goals of AI vs IA, while AI's goal is to replace the HP, the goal of IA is to aid the HP and make him/her more efficient and more accurate.



There are many causes for uncertainty in human medical diagnosis. One approach for quantifying uncertainty in medical diagnosis is to use inter-rater studies, {where} multiple doctors produce diagnostics based on identical medical information without communicating with each other. Associating ground truth with these diagnostics whose inter-rater agreement is less than perfect is challenging.
We briefly describe three categories of problems: {\em signal quality}, the {\em knowledge gap} and the limitations of {\em diagnostic protocols}.}

By {\em Signal Quality} refers to the quality of the raw data
collected for medical diagnosis. Some diagnostic measures, such
as heart rate, blood pressure and temperature can be measured more reliably
and consistently than other
measures such as camera images, X-ray and ultrasound, 
some of which might produce vast and highly variable data. The quality of this data
depends on many factors, including the quality of the instruments, the
consistency of the human operator, the build of the patient, etc.

\input{AlarmFatigue}
%Signal quality enhancement is already an important part of imaging
%devices such as X-ray and MRI. Methods such as compressed
%sensing~\cite{lustig2008compressed} are used to reconstruct 3d images
%from a large number of noisy scans.

One situation where signal quality and signal variability cannot be
neglected is Patient Monitors.  The purpose of these devices is to
continuously monitor patients vital signs and alert HP
if a dangerous situation is detected. Unfortunately, the false alarm
rate of these devices is often high, which leads to ``alarm fatigue'' where the medical staff ignores
the generated alarms, significantly reducing their utility.

Signal quality and alarm fatigue can be thought of as ``bottom up''
causes of uncertainty. The uncertainty originates in the medical
devices and moves up to the HP.

Other types of uncertainty are ``top down'' in that they originate in medical research percolates down to the HP. We briefly
describe two types of top-down uncertainty: knowledge gaps and the
limitation of medical protocols.

{\em ``Knowledge gap''} corresponds to limitations of scientific medical
knowledge. This is not the limitation of a particular doctor; rather,
it reflects the limitations of knowledge that correspond to successful
medical trials.
\input{KnowledgeGap}

Even when medical knowledge exists, an individual doctor might not
have it. The dissemination of up to date and reliable medical
information is uneven. One of the most important information
dissemination tools are {\em medical protocols}. Those are used to
ensure uniformity and consistency of treatment between hospitals,
doctors and nurses. While protocols are an important dissemination
tool, they might not be available for all conditions.

\section{Quantifying uncertainty}

Medical cases vary in their complexity. When the human experts do not agree, it is unlikely that the IA helper will be able to resolve the disagreement. On the other hand, IA is likely to be more helpful on simple case, where doctors are likely to agree. However, the IA needs to {\em know} whether the case is easy or hard, so that it gives advice only when useful. How can the IA quantify it's own confidence?

Increasing confidence in a diagnosis by seeking consensus among
several doctors is common sense. A similar approach has been used in
machine learning algorithms such as 
Bagging~\cite{breiman1996bagging}, Random Forests~\cite{breiman2001random}, and
Boosting\cite{SchapireFr2012}. These so-called {\em ensemble} algorithms
take the majority vote of predictions from several ``base'' learning
algorithms using a majority vote to generate a single more reliable
prediction.
%~\footnote{A majority is used when there are only two possible labels. A more general combination rule is the {\em plurality} i.e., the label that gets the largest number of votes.}

%The simple majority vote always outputs one of the labels. A standard technique for measuring the %confidence of the prediction is to consider the difference in number of votes between the two top %classes (diagnostics). 
%If the difference is large, then the top class is output. If it is small, then the algorithm outputs %IDK.


\section{Augmenting medicine}

\input{ProtocolLimitations}
Our focus so far was on 
the {\em technology}, not on the {\em adoption} of the technology by humans and organizations. While technology can advance very quickly, the adoption of technology is can be slow and difficult.  
We devote this section to discussing the adoption of AI and IA in medicine.

Consider first an AI system developed with the goal of replacing the
caregiver. Naturally, HP will prefer not to use the
system. Will the patient prefer the human HP or the AI system?
Some preliminary studies show that patients trust a human HP more than
technology \cite{ongena2020patients}, and that trust requires a human
to human connection \cite{nundy2019promoting}.

Unlike AI, IA systems do not aim to replace HP. Rather,
they aim to assist HP by automating the simple
cases, and deferring to the human for the more complex cases. In this way IA can 
achieve the goal of ``computer and human work together'' \cite{verghese2018computer}. 
This makes HP more efficient and effective, but does
not take away the HP's agency or humanity. When the IA system outputs
IDK it explicitly passes the responsibility for the
patient to HP.

As discussed earlier, challenging medical cases often fall within gray areas, where
HPs differ on the correct diagnosis or the best treatment. In such
cases HP has the responsibility of making a decision even
though the decision might be wrong. Price et. al. have studied the ethical,
legal and regulatory aspects of using AI in
medicine.\cite{price2014black,ford2016privacy, price2017regulating}
Their conclusion is that the ultimate responsibility for the patients
well-being is {\em always} with the human HP. Even if the AI
system is known to make fewer mistakes than the average doctor in some well-defined tasks,
mistakes are unavoidable, and the question is who is responsible for
the mistake, and who needs to explain their decisions and potentially lose their license to practice.

\iffalse

In this book the point is made that human error is inevitable.
\yoav{ What is the main point of this paper? \cite{donaldson2000err}}

\sout{
Today, and in the foreseeable
future, \sout {\color{blue}non-trivial efforts are needed to convert} medicine \sout{will not be}{\color{blue}to} a precise science. {\color{blue}Even if medicine fulfills precise science,} incorrect decisions {\color{blue}is inevitable due to human natures \cite{donaldson2000err}, and }
can result in harm or death.  }
\fi

We give three perspectives on the possible integration of IA with
medicine: IA and the individual medic, the decentralization of
medicine, and IA as a method for disseminating medical knowledge.

\subsection{IA and the individual HP}

\input{TechnicalVSAdaptive} 
From the perspective of an individual medic, an IA helper is a
tool that augments their diagnostic abilities by increasing accuracy
and by saving time.

To better understand the diagnostic process and the possibilities  of
improving it using IA, we turn to the Kahaneman's~\cite{kahneman2011thinking}
``Thinking Fast Thinking Slow'' and to Vordermark book on medical
decision making~\cite{vordermark2019introduction}. According to these authorities,
medical diagnosis is a combination of two types of processes: {\em
  recognition} and {\em elimination}.

{\em Recognition} is an automatic mental process where one diagnosis
presents itself in the doctors mind as truth. Pathologists,
Radiologist's and other ``Pattern Doctors''~\cite{topol2019deep} make heavy use
of recognition \yoav{\sout{when the task is about recognizing patterns}}. Their experience allows them to quickly sift through
large amounts of data and detect complex patterns. Pattern Doctors \yoav{\sout{handling patterns}}
often work under great time pressure, which can cause them to miss
important patterns. IA can help the doctor by performing a fast
analysis of the signal and alerting the doctor to locations that might
indicate a pathology. This improves the accuracy and speed of the
pathologist while maintaining the responsibility of the doctor to the
final diagnostics. 

As recognition is often a automatic mental process it can be difficult to explain verbally.
This hinders documenting, critiquing and teaching pattern recognition. As
recognition typically points to a single diagnosis, there is a danger
of overlooking other possible diagnoses. IA can serve as a ``note
taker'' documenting the diagnostic process, and pointing out possible
errors of omission. 

An example of a pattern classification problem is the annotation of
sleep. There is active research on using AI to automate this task
\cite{sleepHT2020}.
There is non-trivial inter-rater disagreement rate among experts.  
Existing systems, however, output the identified sleep stages, 
without providing any measure of confidence. Providing a measure of confidence allows the HP to concentrate on the complex parts of the signal, while delegating the easy pars to IAHs. 
Suppose 10\% of the cases result in IDK. HPs could spend only a short amount of time on the remaining 90\% of the cases, and focus on more complex ones.
Such division of labor can increase the likelihood that a pattern HP will  the IA system.

{\em Elimination}, unlike recognition, is a slow deliberative and
verbal process which starts with all possible diagnoses and gradually
eliminates unlikely ones based on patient history, examination and
test results. As Elimination is deliberative, it is easier to discuss,
document and teach it.

An IA system could help the elimination process carefully and
systematically eliminate incorrect diagnoses. This can reduce the chance of overlooking possible diagnostics.

\subsection{IA and Decentralized medicine}

Medicine today is highly centralized. Most interactions
between patient and medic occur in hospitals and clinics. These
large facilities are expensive to build and to maintain. Traveling to
a hospital and back in order to see a doctor for 5 minutes is highly
inefficient. 

Part of the solution is telemedicine. In these days of Covid-19,
telemedicine has gained popularity. Medic and patient can meet in a
virtual space without either leaving home. Moreover, diagnostic
devices can be placed in the patient's home and provide the doctor with
{a record of vital signs during and between meetings.}


IA can surrogate a nurse or technician in telemedicine in two ways. First,
it can guide the patients in placing the sensors on his/her
body so as to get a good signal or image. Second, it can perform an
initial diagnosis. If the patterns are simple, output a diagnosis.
Otherwise, output IDK and alert the remote healthcare provider. 

One significant disadvantage of decentralized medicine is {limited} monitoring, especially when the patient is living alone. 
Falls and other accidents can go undetected for hours, days, or weeks, endangering the patient. IAHs can monitor the patient's activity, detect critical events and alert HP when needed.

Over-centralization is particularly problematic in long-term care.
Seniors are often pressured to move to institutions such as
assisted living so that they are closer to a medical staff. This, in
spite of significant medical, mental and financial cost of such a
move. An approach to long term care called ``aging in
place'' is gaining popularity around the world. IA helpers can help
seniors perform tasks without taking away their agency.


\subsection{IA and knowledge dissemination}

Through machine learning, IA helpers can adapt, over time, to the
medic using them. This is particularly true for  doctors that
perform pattern analysis of complex signals. Initially, the helper will
use some standard set of parameters which gives reasonable performance on
typical signals. Over time, the helper will learn to imitate the
doctor that is using it. This knowledge is captured in the  {\em learned model}.

Learned models, especially those corresponding to experienced and
successful doctors, are likely to be useful for other doctors,
possibly at the beginning of their career. Models from many doctors,
in many institutions, can be collected in repositories. Such models
will have many uses, from initializing helpers for new doctors,
through the integration of many models into a single better model.

IA models complement other methods of medical method dissemination
such as protocols, books, lectures and journal articles. Such models might have the advantage of capturing pattern recognition methods
which are often hard to describe in words. Training a pathologist by interacting with a cancer detection model is likely to be more effective than following a tutorial that explains the same \cite{reid2000medical}.

These models are likely to agree with each other on the easy cases, but are likely to disagree on the harder cases. By comparing the outputs of models trained by multiple experts one can distinguish the easy cases, on which a confident prediction can be made, from the harder cases, on which the diagnosis is IDK, and the doctor needs to use additional tests to arrive at a reliable diagnosis.

\section{About the Authors}
\input{Authors}

\section{References}
% \bibliographystyle{alpha} 
\bibliography{medbib}

\end{document}

