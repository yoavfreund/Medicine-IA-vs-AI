\documentclass[11pt]{pnas-new}
% \documentclass[10pt]{article}
\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{xcolor,ulem}
\usepackage{mdframed,ulem}
\usepackage{wrapfig}
\usepackage{multicol}
\setlength{\columnsep}{1cm}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{lipsum}
\newlength{\strutheight}
%\usepackage{soul} % for strike-through (\st)

\author[1]{Hau-Tieng Wu}
\author[2]{Yoav Freund}

\affil[1]{Duke, Mathematics and Statistical Science, Durham, 27708, USA}
\affil[2]{UCSD, Computer Science, San Diego, 92093, United States. yfreund@eng.ucsd.edu}


\title{Trusted digital doctors know their limits}

\input{macros}
\renewcommand{\input}[1]{}

\begin{abstract}

  The meteoric rise of AI in general and Deep Learning in particular
  is generating great excitement throughout academia and commerce, and
  in particular in medicine\cite{topol2019deep,
    wachter2015digital}. With some some high-profile claims
  that AI will soon replace humans in many medical specialties.

  In this position paper we present an alternative view. We contrast
  {\em Artificial Intelligence} with {\em Intelligence Augmentation}
  and argue that the second is more likely to benefit the patient than
  the first. We provide evidence to this argument and present a vision
  in which easier decisions are delegated to computers, while the more
  difficult ones are handled by humans.

\end{abstract}

\begin{document}
\settoheight{\strutheight}{\strut}

 
\maketitle

%\thispagestyle{firststyle}
\iffalse
\section{issues to be resolved}

\hautieng{As we discussed, I followed the medical journal convention and changed the author order so that you are the senior/corresponding author.}

\begin{itemize}
    \item Introduce the term "Healthcare Provider" with acronym HP, and use throughout instead of doctors nurses etc. \hautieng{I am not sure what is the best way to do it. In many places, "doctor" are clearly more precise than HP. So I leave this part to you.}
    \item Patient compliance - how can IA help make sure that patients take their meds, don’t drink excessively etc. \hautieng{done}
    \item Compensation: Doctors that generate quality data should own this data, distribution mechanisms should compensate the doctor for his/her contribution. \hautieng{As we discussed, this might be a bit off the topic. This is more like a regulatory issue. Let's discuss it if you have a different viewpoint.}
    \item A few grammatical corrections:
    -insert 2 page 2 last line “piopsies” instead of biopsies
    -last line page 2 spelling: “therefor” (needs an e) therefore 
    -insert page 7 we quote from Robert Rechter’s book... (53) Reference @ 53 indicates “Robert Wachter” as author. \hautieng{I am not fully sure what you want...}

\end{itemize}

To Highlight a change use this macro: \change{old text}{new text}
\fi

\section{Introduction}

The meteoric rise of  Artificial Intelligence (AI) and Deep learning (DNN) raises the possibility that
doctors will be replaced by computers~\cite{Mukherjee2017}. Geoff Hinton,
a famous deep learning researcher said in 2017: ``It's just completely
obvious that in ten years deep learning is going to do better than
Radiologists ... They should stop training radiologists now''.

The predictions of Sebastian
Thrun~\cite{Mukherjee2017,esteva2017dermatologist}, another leader in
machine learning, are less disruptive: ``... deep learning devices
will not replace dermatologists and radiologists. They will {\em
  augment} professionals, offering the expertise and assistance''. 
  The term "Augmented Intelligence" appears frequently in recent 
  policy reports from medical associations~\cite{american2019augmented,AAD2019augmented}. Collaborations between computers and human care-givers are seen as a desirable goal.
  In this article we suggest how such a collaboration between human and computer might work in practice. Central to our approach is to give the computer the ability to quantify it's own uncertainty. When the uncertaintyis high, the computer will output "I don't know" (which we will abbreviate as IDK).
 
\input{AIIABox} The argument as to whether human healthcare providers (HP)~\footnote{Our discussion applies to all healthcare providers, including Doctors, Nurse Practitioners and Nurses. To simplify the references to this group of professionals we use the acronym HP, which stands for "Healthcare Provider"}  will be replaced or empowered by computers is part of a wider debate between AI, whose goal is to imitate human behaviour  and Intelligence Amplification (IA), whose goal is to extend human capabilities. Computers with AI are sometimes called "AI agents", we contrast computers with IA by referring to them as "IA Helpers" (IAH). Helpers are expected to help when they can, and output IDK when they cannot. 
Of the many potential uses of AI/IA in medicine, we focus on diagnostics.
AI-driven diagnosis are expected to become a reality much sooner than other medical activities such as surgery~\cite{topol2019deep}. 

Central to our argument is a quantification of {\em prediction
  confidence}. Such quantification is needed to avoid premature
diagnostic conclusions, and to decide which additional tests or
consultations might be needed. Consider a doctor that is asked
to diagnose a patient with complex or conflicting symptoms. A careful
doctor will acknowledge her uncertainty and order additional tests or
consult a specialist. A less careful, more self confident doctor is
more likely to give an incorrect diagnosis or choose an ineffective or even damaging treatment plan.

An AI agent, expected to be better than the human HP, is more likely to 
end up behaving like an overly confident doctor. An IAH, aware of
its own limitations, will give advice only when the evidence is
strong and otherwise say IDK.

In the following we explore these ideas. We
start with a critique of one of the papers claiming that AI agents can outperform human diagnosticians.

\section{Black-box learning}
\label{sec:ground-truth}

Deep learning (DNN) is a powerful black-box learning algorithm. Examples of applications of DNN to medical diagnosis include arrhythmia classification 
from single channel electrocardiogram \cite{hannun2019cardiologist}, diabetic retinopathy detection from retinal fundus photographs \cite{gulshan2016development}, skin cancer diagnostics~\cite{esteva2017dermatologist} as well as many others.

\input{Supervised}
%The data for supervised learning consists of a large collection of
%(input, output) pairs. For medical diagnosis, usually the input is medical
%information for the patient (Heart rate, blood tests, X-ray images
%etc.) and the output is the diagnosis. This output is output is assumed to be the undisputed
%``ground-truth''. Unfortunately, this assumption is problematic in the context of medical diagnostics. 
%It is not uncommon that different diagnosticians give different diagnoses for the same medical %information which 
%makes it hard to assign a ground to each instance. We will return to this important issue in the next %section.

%The other important assumption made in supervised learning is that the
%generated classifier is tested using the same distribution of examples
%as that of the training set. In practice, however, this is often hard to achieve.

\input{SkinCancer}
In a highly cited paper in the journal Science~\cite{esteva2017dermatologist} the authors 
claim that DNNs can perform skin cancer diagnostics as well as, or better, than board certified dermatologists. 

A fundamental problem with the experiments presented in the paper is that the test data used does not represent 
a realistic deployment of the system.
The data used in the experiment was {\em retrospective},
i.e., it was collected from the records of past patients for which both
a skin image and a biopsy were available. Normally, patients get
biopsied only if the dermatologist thinks there is a significant
chance of malignancy. As a result, a retrospective study that is
based on patients for whom a biopsy was taken is likely to
over-represent malignant patients and therefore be biased. If an image-based classifier
is trained on the biased data, its performance on unbiased test data
is likely to be worse. Specifically, when the classifier is applied to skin
images of un-diagnosed patients it is likely to over-diagnose them as
malignant, potentially resulting in an increase of the number of unnecessary biopsies.

\section{Uncertainty in medicine}

Uncertainty is prevalent in medical diagnostics 
while  {\em ground truth} is often hard to find.
Diagnosis is not a simple input-output mapping. Rather, it is an iterative process that reduces uncertainty over time. For "easy" cases, the process starts and ends within a single visit to an HP. 
In harder cases, a referral is made to a specialist.  A small fraction of the cases are very hard, such cases require numerous  tests and consultations before a diagnostics and a treatment plan is chosen. Diagnosis is less like a single 
classifier, it is more similar to a cascade of classifiers, each of which sometimes outputs IDK, leaving the decision to the next classifier.

Consider the concept of "ground truth", can we determine whether or not a diagnosis was correct? The answer is "sometimes".
Sometimes, for particular diseases and particular treatments, there are well known symptoms for particular disease/treatment pairs, such symptoms can be used to confirm or deny that the diagnosis was correct. 
In many cases, However, correct diagnosis is only one of many 
factors that influence the health and symptoms of the patient. Other factors include choice of treatment, medication adherence, changes in living and working environments, diet, stress, etc. In those cases it is hard to assign a diagnosis a label of correct or incorrect. We simply have no access to the ground truth.

One way to measure the difficulty of a case is to ask multiple HP to give a diagnosis. If they all give the same diagnosis, then the case is easy, if there is significant differences, the case is hard.  
We argue that the goal of learning should be to predict correctly on the easy cases and output IDK on the hard cases. In other words, we change the goal from performing better than the best HP to that of performing almost as well as the consensus among HPs. 
%
This reflects the different goals of AI vs IA, while the goal of AI is replacing the HP, the goal of IA augmenting the HP
by making him/her more efficient and more accurate.
There is extensive research in quantifying uncertainty in medical diagnosis. One approach is inter-rater agreement tudies, 
where multiple doctors produce diagnostics based on identical medical information and without communicating with each other. 
One way to frame the goal of IA is to diagnose those cases on which the inter-rater agreement is high. 

Many problems hinder the human diagnostician.
We briefly describe three categories of problems that are relevant to our discussion: {\em signal quality}, the {\em knowledge gap} and the limitations of {\em diagnostic protocols}.

By {\em Signal Quality} refers to the quality of the raw data
collected for medical diagnosis. Some diagnostic measures, such
as heart rate, blood pressure and temperature can be measured more reliably
and consistently than other
measures such as camera images, X-ray and ultrasound, 
some of which might produce vast amounts of highly variable data. The quality of this data
depends on many factors, including the quality of the instruments, the
consistency of the human operator, the body of the patient, etc.

\input{AlarmFatigue}
%Signal quality enhancement is already an important part of imaging
%devices such as X-ray and MRI. Methods such as compressed
%sensing~\cite{lustig2008compressed} are used to reconstruct 3d images
%from a large number of noisy scans.

One situation where signal quality and signal variability cannot be
neglected is Patient Monitors.  The purpose of these devices is to
continuously monitor patients vital signs and alert the HP
if a dangerous situation is detected. Unfortunately, the false alarm
rate of these devices is often high, which leads to ``alarm fatigue'' where the medical staff ignores
the generated alarms, significantly reducing their utility.

Signal quality and alarm fatigue can be thought of as ``bottom up''
causes of uncertainty. The uncertainty originates in the medical
devices and moves up to the HP.

Other types of uncertainty are ``top down'' in that they originate in medical research and 
percolates down to the HP. We briefly
describe two types of top-down uncertainty: knowledge gaps and the
limitation of medical protocols.

{\em ``Knowledge gap''} corresponds to limitations of scientific medical
knowledge. This is not the limitation of a particular doctor; rather,
it reflects the limitations of knowledge that correspond to successful
medical trials.
\input{KnowledgeGap}

Even when medical knowledge exists, an individual doctor might not
have it. The dissemination of up to date and reliable medical
information is uneven. One of the most important information
dissemination tools are {\em medical protocols}. Those are used to
ensure uniformity and consistency of treatment between hospitals,
doctors and nurses. While protocols are an important dissemination
tool, they might not be available for all conditions.

\section{Quantifying uncertainty}

Medical cases vary in their complexity. When the human experts do not agree, it is unlikely that IAH will be able to resolve the disagreement. On the other hand, IAH is likely to be more helpful on simple cases, where doctors are more likely to agree. However, IAH needs to {\em know} whether the case is easy or hard, so that it gives advice only when useful. How can IAH quantify its own confidence?

Increasing confidence in a diagnosis by seeking consensus among
several doctors is common sense. A similar approach has been used in
machine learning algorithms such as 
Bagging~\cite{breiman1996bagging}, Random Forests~\cite{breiman2001random}, and
Boosting\cite{SchapireFr2012}. These so-called {\em ensemble} algorithms
take the majority vote of predictions from several ``base'' learning
algorithms using a majority vote to generate a single more reliable
prediction.
%~\footnote{A majority is used when there are only two possible labels. A more general combination rule is the {\em plurality} i.e., the label that gets the largest number of votes.}

%The simple majority vote always outputs one of the labels. A standard technique for measuring the %confidence of the prediction is to consider the difference in number of votes between the two top %classes (diagnostics). 
%If the difference is large, then the top class is output. If it is small, then the algorithm outputs %IDK.


\section{Augmenting medicine}

\input{ProtocolLimitations}
Our focus so far was on IA/AI as {\em technologies}, we now turn our sights to  the {\em adoption} of IA/AI technology by people and organizations. While technology can advance very quickly, the adoption of technology can be slow and difficult.  

Consider first an AI system developed with the goal of replacing the
HP. 
Will the patient prefer the human HP or the AI system?
Some preliminary studies show that patients trust a human HP more than
technology \cite{ongena2020patients}, and that trust requires a human
to human connection \cite{nundy2019promoting}. Also, the HP might prefer not to collaborate with a system that threatens his/her livelihood.

Unlike AI, the goal of IA is not to replace the HP. Rather,
it aims to help the HP make diagnoses. IAH interacts with the HP, not with the patient. In this way IA can 
achieve the goal of ``computer and human work together'' \cite{verghese2018computer}. 
This makes HP more efficient and effective, but does
not take away the HP's agency or humanity. 

As discussed earlier, challenging medical cases often fall within gray areas, where
HPs differ on the correct diagnosis or the best treatment. In such
cases HP has the responsibility of making a decision even
though the decision might be wrong. Price et. al. have studied the ethical,
legal and regulatory aspects of using AI in
medicine.\cite{price2014black,ford2016privacy, price2017regulating}
Their conclusion is that the ultimate responsibility for the patients
well-being is {\em always} with the human HP. Even if the AI
system is known to make fewer mistakes than the average doctor in some well-defined diagnostic tasks.
Some mistakes will happen, in which case the question is who bears the responsibility,
who needs to explain their decisions in the court of law and who can potentially lose their license to practice. 
Responsibility holds no meaning for a non-human AI or IA.

\iffalse

In this book the point is made that human error is inevitable.
\yoav{ What is the main point of this paper? \cite{donaldson2000err}}

\sout{
Today, and in the foreseeable
future, \sout {\color{blue}non-trivial efforts are needed to convert} medicine \sout{will not be}{\color{blue}to} a precise science. {\color{blue}Even if medicine fulfills precise science,} incorrect decisions {\color{blue}is inevitable due to human natures \cite{donaldson2000err}, and }
can result in harm or death.  }
\fi

We give three perspectives on the possible integration of IA with
medicine: IA and the individual medic, the decentralization of
medicine, and IA as a method for disseminating medical knowledge.

\subsection{IA and the individual HP}

\input{TechnicalVSAdaptive} 
From the perspective of an individual medic, IAH is a
tool that augments their diagnostic abilities by increasing accuracy
and by saving time.

To better understand the diagnostic process and the possibilities  of
improving it using IAH, we turn to the Kahaneman's~\cite{kahneman2011thinking}
``Thinking Fast Thinking Slow'' and to Vordermark book on medical
decision making~\cite{vordermark2019introduction}. According to these authorities,
medical diagnosis is a combination of two types of processes: {\em
  recognition} and {\em elimination}.

{\em Recognition} is an automatic mental process where one diagnosis
presents itself in the doctors mind as truth. Pathologists,
Radiologist's and other ``Pattern Doctors''~\cite{topol2019deep} make heavy use
of recognition. Their experience allows them to quickly sift through
large amounts of data and detect complex patterns. Pattern Doctors 
often work under great time pressure, which can cause them to miss
important patterns. IAH can help the doctor by performing a fast
analysis of the signal and alerting the doctor to locations that might
indicate a pathology. This improves the accuracy and speed of the
pathologist while maintaining the responsibility of the doctor to the
final diagnostics. 

As recognition is often a automatic mental process it can be difficult to explain verbally.
This hinders documenting, critiquing and teaching pattern recognition. As
recognition typically points to a single diagnosis, there is a danger
of overlooking other possible diagnoses. IAH can serve as a ``note
taker'' documenting the diagnostic process, and pointing out possible
errors of omission. 

An example of a pattern classification problem is the annotation of
sleep. There is active research on using AI to automate this task
\cite{sleepHT2020}.
There is non-trivial inter-rater disagreement rate among experts.  
Existing systems, however, output the identified sleep stages, 
without providing any measure of confidence. Providing a measure of confidence allows the HP to concentrate on the complex parts of the signal, while delegating the easy pars to IAHs. 
Suppose 10\% of the cases result in IDK. HPs could spend only a short amount of time on the remaining 90\% of the cases, and focus on more complex ones.
Such division of labor can increase the likelihood that a pattern HP uses the IAH.

{\em Elimination}, unlike recognition, is a slow deliberative and
verbal process which starts with all possible diagnoses and gradually
eliminates unlikely ones based on patient history, examination and
test results. As Elimination is deliberative, it is easier to discuss,
document and teach it.

IAH could help the elimination process carefully and
systematically eliminate incorrect diagnoses. This can reduce the chance of overlooking possible diagnostics.

\subsection{IA and Decentralized medicine}

Medicine today is highly centralized. Most interactions
between patient and medic occur in hospitals and clinics. These
large facilities are expensive to build and to maintain. Traveling to
a hospital and back in order to see a doctor for 5 minutes is highly
inefficient. 

Part of the solution is telemedicine. In these days of Covid-19,
telemedicine has gained popularity. HP and patient can meet in a
virtual space without either leaving home. Moreover, diagnostic
devices can be placed in the patient's home and provide the doctor with
{a record of vital signs during and between meetings.}


IAH can surrogate a nurse or technician in telemedicine in two ways. First,
it can guide the patients in placing the sensors on his/her
body so as to get a good signal or image. Second, it can perform an
initial diagnosis. If the patterns are simple, output a diagnosis.
Otherwise, output IDK and alert the remote healthcare provider. 

One significant disadvantage of decentralized medicine is {limited} monitoring, especially when the patient is living alone. 
Falls and other accidents can go undetected for hours, days, or weeks, endangering the patient. IAH can monitor the patient's activity, detect critical events and alert HP when needed.

Over-centralization is particularly problematic in long-term care.
Seniors are often pressured to move to institutions such as
assisted living so that they are closer to a medical staff. This, in
spite of significant medical, mental and financial cost of such a
move. An approach to long term care called ``aging in
place'' is gaining popularity around the world. IAH can help
seniors perform tasks without taking away their agency.


\subsection{IA and knowledge dissemination}

Through machine learning, IAH can adapt, over time, to the
medic using them. This is particularly true for  doctors that
perform pattern analysis of complex signals. Initially, the helper will
use some standard set of parameters which gives reasonable performance on
typical signals. Over time, the helper will learn to imitate the
doctor that is using it. This knowledge is captured in the  {\em learned model}.

Learned models, especially those corresponding to experienced and
successful specialists, are likely to provide useful help other doctors,
possibly at the beginning of their career or at a rural hospital. Models from many doctors,
in many institutions, can be collected in repositories. Such models
will have many uses, from initializing helpers for new doctors,
through the integration of many models into a single better model.

IA complements other methods of medical method dissemination
such as protocols, books, lectures and journal articles. Such models might have the advantage of capturing pattern recognition methods
which are often hard to describe in words. Training a pathologist by interacting with a cancer detection model is likely to be more effective than following a tutorial that explains the same \cite{reid2000medical}.

These models are likely to agree with each other on the easy cases, but are likely to disagree on the harder cases. By comparing the outputs of models trained by multiple experts one can distinguish the easy cases, on which a confident prediction can be made, from the harder cases, on which the diagnosis is IDK, and the doctor needs to use additional tests to arrive at a reliable diagnosis.

%\section{About the Authors}
%\input{Authors}

\section{References}
% \bibliographystyle{alpha} 
\bibliography{medbib}

\end{document}

