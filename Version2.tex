\documentclass[9pt,twocolumn,twoside]{pnas-new}
% \documentclass[10pt]{article}
\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{mdframed}
%\usepackage{wrapfig}
\usepackage{multicol}
\setlength{\columnsep}{1cm}

\author[1]{Yoav Freund}
\author[2]{Hau-Tieng Wu}
\affil[1]{UCSD, department, city, postcode, country}
\affil[2]{Duke, department, city, postcode, country}

\title{When the digital Doctor should admit\\ "I don't know"}

\input{macros}

\begin{abstract}

  The meteoric rise of AI in general and Deep Learning in particular
  is generating great excitement throughout academia and commerce, and
  in particular in medicine\cite{topol2019deep,
    wachter2015digital}. With some some high-profile claims~\cite{}
  that AI will soon replace humans in many medical specialties.

  In this position paper we present an alternative view. We contrast
  {\em Artificial Intelligence} with {\em Intelligence Augmentation}
  and argue that the second is more likely to benefit the patient than
  the first. We provide evidence to this argument and present a vision
  in which easier decisions are delegated to computers, while the more
  difficult ones are handled by humans.

\end{abstract}

\begin{document}

\maketitle

\thispagestyle{firststyle}

\section*{Introduction}

Digital technology is causing a sea-change in all parts of the medical
profession. In particular the meteoric rise of AI in general and deep
learning in particular raises the possibility that doctors will be
replaced computers~\cite{Mukherjee2017}. The father of deep learning,
Geoff Hinton, said in 2017: "It's just completely obvious that that in
ten years deep learning is going to do better than Radiologists
... They should stop training radiologists now".

Other deep learning researchers provide a more nuanced
perspective. Sebastian
Thrun~\cite{Mukherjee2017,esteva2017dermatologist} argues that
"... deep learning devices will not replace dermatologists and
radiologists. They will {\em augment} professionals, offering the
expertise and assistance".

\Org{Artificial Intelligence and Intelligence Augmentation}{
Using computers to augment human intelligence rather replace it is
both tantalizing and mundane. On the heady side, consider
cyborgs whose anatomy is part human, part artificial and can with
equal ease solve complex equations or write poetry. On the mundane
side, think of smartphones that are quickly becoming an inseparable
part of our person.
 
The idea of using computers to augment or amplify human intelligence
has a very long history. The acronyms AI (Artificial intelligence) and
IA (Intelligence Amplification or Intelligence Augmentation) have both
become popular in the early
1960's\cite{ashby1957introduction,engelbart1962augmenting}. These
days, the acronym AI is popular, while the acronym IA is not. However,
Sebastian Thrun's statement indicates that the idea of Intelligence
augmentation is still on people's mind.}

{\bf What would IA look like when applied to medicine?} that is the
question we aim to answer here.  We argue that an important ingredient
of the answer is to introduce to AI agents a level of
humility. Specifically, to design classifiers, such as DNNs, to say "I
don't know".

\section*{Labels, ground truth and testing}

\ML{Supervised Learning and ground truth}{Roughly
  speaking, machine learning (ML) can be divided into {\em
    unsupervised} learning and {\em supervised} learning. In both, the
  task of the learning algorith is transform a set of {\em examples}
  into a {\em model}. In unsupervised learning the examples are
  undifferentiated raw measurements. In {\em supervised} learning,
  which is the focus of this article, each example consists of an {\em
    input} and a {\em label}. Typically, the labels are provided by a
  human expert. These labels define the {\em ground truth} and the
  goal of the learning algorithm is to make predictions that diverge
  as little as possible from the ground truth.}


\ML{Skin cancer diagnosis using Deep Neural Networks}{One of the
  papers that provided evidence that deep neural networks might be
  able to outperform humans is the work of Esteva et
  al~\cite{esteva2017dermatologist}. They trained a Deep neural
  network to classify images of skin into three categories: benign,
  malignant and non-cancerous. The network was then tested, along with
  twenty five dermatologists on images which were labeled by a
  pathologist analysis of the biopsy. The neural network outperformed
  the human dermatologist. This is, without a doubt, an impressive
  finding. However, it is based on a retrospective analysis, in other
  words, an analysis of historical data. To predict the performance of
  the DNN when used in a dermatology practice we need to how a
  dermatologist, or any other diagnosticians, arrives at their final
  diagnostics.}

In their famous work, Esteva et al. set out to show that a classifier
trained by machine learning can performs as well as or better than
expert dermatologists.  In this application of supervised learning
each example consists of an input image of a skin patch and an output
label that is ``benign'' or ``melignant''

As they wanted to compare the system to human dermatologists they
needed a better ground truth than that provided by the dermatologists.
To that end they used the diagnosis of a biopsy as ground truth. It is
arguable that this label is more accurate than the one given by the
dermatologist, even though it depends on the human judgement of the pathologist.

However, even if we assume that pathologists labels are more reliable
than dermatology labels, the requirement that each example corresponds
to a biopsy introduces a significant bias. Under normal circumstances, patients get
biopsied only if the dermatologist thinks there is a chance of
{\color{blue}malignancy. Therefore}, the set of biopsied examples is biased towards
{\color{blue}malignancy}. It is likely that using a classifier trained in this way
on an unfiltered stream of patients will increase the number of
patients unnecessarily getting a biopsy.

\section*{Uncertainty in medicine}

\yoav{Please explain citation: \cite{Johnson2016} \cite{fleischman2019emergency}}
\Medicine{Patient Monitoring}{
  \hautieng{ patient monitor is an integrated system equipped with various bio-sensors that record, display and distribute different biometrics, ranging from vital signs like heart rate, oxygen saturation, blood pressure, and high-frequency waveforms like electrocardiogram, respiratory signal, arterial blood pressure. It is typically used in hospitals and clinics to closely monitor patients for a high quality patient care. Usually, a patient monitor comes with an alarm system that alerts clinicians life-threating clinical events, like asystole, ventricular fibrillation, or an intubated patient disconnects from the ventilator, among many others. However, usually such system is not adapted to the patient, and a lot of false alarms lead to one major healthcare concern called {\em alarm fatigue} (or alarm overload) (cite ECRI 2020). In addition to solving this alarm fatigue issue, there have been soaring research interests in developing an intelligent diagnostic system from taking as much waveform signals as possible into account. }
  \yoav{Say a few words about what is an itelligent alarm
    system. Does it adapt to the patient. Is this related to alarm
    Fatigue?}
  However, it has been long debate if the recorded biosignals are suitable for this purpose, due to its ``blackbox'' nature \cite{Feldman2006,Shelley2016,Cannesson2016}. {\color{blue}See below for an example.}
\yoav{Is the problem with the biosignals or with the system? Regarding black-boxes, as long as we allow the box to output I don't know when the prediction is unstable. Also, per-example explanations can be derived from the features that contribute the most from the score, assuming the learner is biased towards sparse classifiers classifier is a sparse one, such as boosting or Lasso.}
}

\Medicine{Blood pressure monitoring}{
\hautieng{To appreciate the ``black box'' issue, let us look at this example.}
\yoav{I believe the common interpretation of a "black-box model" is a model that relates to the system we are trying to predict only in the input/output sense. A clear-box system is one where the internal variable relate to measurable quantities. A clear-box system might still have parameters that need to be calibrated, noise issues etc.
I would prefer to use black box in the narrow sense, and separately from calibration, noise, or artifacts.}
Recently, some delicate artifacts
  have been reported \cite{lin2019unexpected} regarding the pulse
  transit time {\color{blue} (PTT) analysis. PTT is defined to be the phase latency between the cycles in the electrocardiogram and the photoplethysmogram. It} has been shown that {\color{blue}PTT contains rich information abotu the} blood pressure \cite{gesche2012continuous}. {\color{blue}It is thus natural to include it to an intelligent system, by learning how it is related to clinical outcomes, to more closely monitor the hemodynamics. However, it was unintentionally found that in some patient monitors, the PTT is contaminated by a sawtooth artifact. Such artifacts might confuse the intelligent system, and lead to unpredictable uncertainties.} \yoav{the
    way this is written, it sounds off topic. Is there an aspect of
    automation or ML that will connect it to the article?}  }

\Medicine{Inter-rater agreement}{
in some cases, physicians can achieve a reliable
  decision making, probably with sufficient clinical information
  \cite{mehta2011agreement} or if only the major information is needed
  \cite{atiya2003interobserver}, {\color{blue} in many other cases, the decision making might not be consistent. This inconsistency is usually understood as the inter-rater agreement, and the degree of inconsistency depends on the situations. For example, the inter-rater agreement rate of identifying ventricular premature contractions from a 12 leads electrocardiogram is high among well-trained cardiologists, but it will be lower if only a single lead electrocardiogram is available in the ambulatory environment.}

  \yoav{Do we need to explain that sometimes inter-rater agreement is high? }}

  \Medicine{Identifying sleep stages}{{\color{blue}Even with sufficient information, it is not guaranteed that the inter-rater agreement rate will be high.} The American Academy of Sleep
    Medicine (AASM) publishes criteria for manual sleep stage
    annotation and
    sleep apnea detection. This annotation is based on manual analysis
    of biosignals \cite{Iber2007,berry2012aasm}. However, it is well known
    that the inter-rater agreement rate of sleep stage annotation
    among experienced experts is only about 76\% over normal subjects
    and about 71\% over subjects with sleep apnea
    \cite{norman2000interobserver}. {\color{blue}Among many reasons, the one that is directly related to the intelligent system development is how the criteria are ``described''. For example, if the delta wave occupies more than 20\% of a given 30-second epoch of the electroencephalogram during sleep, that 30-second epoch is defined to be in the N3 stage. 20\% of a given 30-second epoch is 6 seconds. What about if the delta wave occupies 5.99-, or 6.01-seconds? What about if the delta wave sustains for 10 seconds, but it is divided into two consecutive 30-second epochs? } {\color{blue}In short,} physicians need to make a
  decision when sitting on the ``gray area'' that is not covered by
  the protocol. Over this gray area, different physicians may make
  different decisions based on their experience or the information
  they have at hand. 
  }

  \Medicine{Low inter-rater agreement}{  
  {\color{blue}In many other situations, low inter-rater agreement can
    be found in many clinical problems. Sometimes, the low agreement comes from the ``extrapolation error''; that is, when we apply the developed protocol to the population different from the population that we collect the evidence for the protocol \cite{brosnan2015modest}. 
    In other situation, the variability among subjects is so big that it limits the development of a more quantitative protocol \cite{venhola2003interobserver}. In some situations, when the needed information is missing, it is challenging to make a differential diagnosis
    \cite{moncada2011reading}.
    
    Thus, depending on the problem, sometimes human experts' labels are uncertain, and it is not clear which one is ``correct'', or even how to define ``correct''. This uncertainty piles on another layer of challenge when we train the intelligent system.
    }
    
    \yoav{Please elaborate, what are the clinical problems we are
      discussing here?  Can we consolidate this into one box about
      inter-rater agreements that contains a definition and examples
    of low and high agreement}}

   {\color{blue} In short, the issue underlying the inter-rater agreement is
  subtle. It ranges from the limited physiological knowledge, protocol definition, available
  information, treatment target, economic consideration, etc. A direct consequence of the low inter-rater agreement rate is that the trained intelligent system might be questionable. 
  
Should we trust such a questionable intelligent system? Is there anything useful from such system? To answer this question, we should not forget that physicians also make mistakes. Depending on the problems, and the population of physicians under consideration, the agreement rate varies. Usually, intern doctors know the least, while a senior attending knows the most. We trust a senior expert more, but it does not mean that we do not trust a junior intern doctor. The boundary of trust or not is ``simple'', if you can know how sure the physician is. It is clearly challenging to know how sure the physician is. But when we train an intelligent system, we can easily quantify how sure the system is about a question -- usually it is the score calculated by the system from the input data. In a binary classification problem, the higher or the lower the score is, the more certain the system is about the answer to the input data. When the score is in the middle, we could interpret that the system is {\em not sure} about the answer to the input data, or {\em it doesn't know}.

  }

\iffalse
  \Medicine{Inter-Rater agreement}{
  A direct consequence of this low inter-rater agreement is a
  questionable trained ``artificial intelligence''. It is possible
  that we magically obtain a dataset that contains information that is
  sufficient for the decision making, while the information is too
  subtle so that it is not considered in the protocol, and we also
  magically obtain labels from a magical master that can see though
  all the information and provide the correct decision. However, by
  doing a simple math, we shall not count on such a magic and should
  come back to the protocol itself.

  \yoav{ Can you describe a particular interesting / illuminating /
    convincing case?}
}
\fi


{\bf Sources of uncertainty in medical diagnosis.}
\begin{itemize}
  \item{\bf The diagnostic process of elimination}
  \item{\bf Data Quality, Calibration, resolution} Discuss issue as
    placement of sensors, lighting when analyzing skin lesions. Sensing back for re-testing.
  \end{itemize}

 {\bf Hiding Uncertainty}
  \begin{itemize}
    \item {\bf Psychological reasons} Both doctor and patient prefer
      the projection of certitude.
    \item {\bf Protocols}
    \item {\bf diagnostic devices} Secrecy of the internal code limits
      the trustworthiness of the alarms.
    \item{\bf Alarm Fatigue}
  \end{itemize}

  {\bf Quantifying Confidence} With experience comes confidence. In
  other words, diagnostic options that contradict the accumulated
  experience are eliminated.
  
  {\bf confidence through aggregation}
  \begin{itemize}
    \item A good way to increase diagnostic certainty is to solicit
      the experience of a diverse group of doctors.
    \item If there is a clear majority for one diagnostic outcome,
      then the overall confidence in the diagnostics is high.
    \item This certainty is very different from the the conditional.
      probability of the disease given the diagnostic. The first is
      akin to saying: 95\% of the dermatologists would give the same
      diagnostics. The second defines the probability that, if we had
      access to ground truth, then 95\% of the patients that recieve
      this diagnostics have the corresponding condition.
    \end{itemize}
    
\section*{Uncertainty in Machine Learning}

One can define ``confidence'' in machine learning. The definition follows a
similar logic to the one used for human diagnosticians in the previous
section. The yardstick by which we measure confidence of predicting a
label is ``how much do alternative labels contradict previous
experience?''.
More formally, we ask how much do we need to change the training data
so that it supports an alternative label.



\ML{Uncertainty vs. accurracy}{using ROC curves}

\begin{itemize}
  \item Bootstrap samples.
  \item Samples from different hospitals.
  \item Easy and hard cases.
  \end{itemize}

  \section*{Agency and Augmentation}

  Computers are already an integral part of medicine, from
  electronical medical records to medical instrumentation to billing,
  hospitals and cliniques cannot function without computers. By some
  measures computers can already make better diagnosis that human
  doctors. The question is not {\em whether} computer diagnostics will
  become part of medical practice, the question is {\em how}.

  It is not enough to describe the desired end state. 
  In this final section we chart a {\em migration path} from the
  current limited role of computers in diagnostics to a
  more central role. For this to take place, caregivers must benefit
  from the new technology. Setting the goal to be replacing human
  doctors with machines is both unrealistic and self-defeating.

  Doctors and nurses are humans, they are not diagnostic machines. The
  personal and emotional connection between doctor and patient is
  critical for effective treatment. A good doctor combines their
  medical knowledge with a personal understanding of the patient to
  choose a treatment plan and discuss it with the patient to get their
  consent.

  It is debatable whether a computer will ever be able to
  make a meaningful emotional connection with a patient. It is quite
  clear that such capabilities will not exist in the forseable future.
  We refer to the ability to connect and to act in a self concious
  way {\em agency} and separate it from {\em intelligence}. We suggest
  that computers can augment humans in intelligence tasks and leave
  agency to humans.

  \subsection*{Easy and hard diagnoses}

  As described above, diagnostics is a process of elimination. It
  starts with a set of possible diagnoses which are gradually
  eliminated as evidence is  gathered. As diagnoses are
  eliminated, the benefit and risk of different treatment plans is evaluated.
  Deciding on treatment and continually monitoring it is where the
  doctor's agency is most important.

  Our suggestion is that machine learning helps the doctor eliminate
  diagnoses and evaluate treatment options, while leaving the the
  decisions to the doctor.

  Diagnosis vary in their difficulty. Consider a sequence of
  patients visiting a clinic. Suppose the clinic has four doctors, two
  trainees, and six nurses. Each patient first gets seen by a nurse,
  then by a doctor, and possibly by a trainee. Some of patients have
  a simple diagnosis, one that all twelve members of the clinic will
  state with confidence. Other patients have more complex diagnosis
  that requires the attention of a doctor. Finally, some diagnosis are
  so complex that the doctor needs to consult other doctors and
  possibly ask for additional tests. In other words, the clinic acts
  as an ensemble of classifiers. Easy cases result in a unanimous
  diagnosis, harder ones results in a clear majority, and the very
  hardest results in disagreement which requires consultation and
  additional tests.

  The machine learning ensemble of classifiers follows a similar
  logic. 

\begin{itemize}
\item{\bf Computer aided diagnostics}
  Especially with very large data: ecg for 14 says.... \\
  ~\\

Pathology.

\item {\bf Dissemination of expertise}
Computers, trained by experts, can help novices.  Serves a function
similar to score-cards.

Teaching young diagnostics
\item { \bf Confidence, Trust and adoption of technology}
\end{itemize}

\section*{Summary}

%\bibliographystyle{alpha} 
\bibliography{medbib}

\end{document}