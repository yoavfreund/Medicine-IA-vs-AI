\documentclass[9pt,twocolumn,twoside]{pnas-new}
% \documentclass[10pt]{article}
\templatetype{pnasresearcharticle} % Choose template 
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{mdframed}
%\usepackage{wrapfig}
\usepackage{multicol}
\setlength{\columnsep}{1cm}

\author[1]{Yoav Freund}
\author[2]{Hau-Tieng Wu}
\affil[1]{UCSD, department, city, postcode, country}
\affil[2]{Duke, department, city, postcode, country}

\title{When the digital Doctor should admit\\ "I don't know"}

\input{macros}

\begin{abstract}

  The meteoric rise of AI in general and Deep Learning in particular
  is generating great excitement throughout academia and commerce, and
  in particular in medicine\cite{topol2019deep,
    wachter2015digital}. With some some high-profile claims~\cite{}
  that AI will soon replace humans in many medical specialties.

  In this position paper we present an alternative view. We contrast
  {\em Artificial Intelligence} with {\em Intelligence Augmentation}
  and argue that the second is more likely to benefit the patient than
  the first. We provide evidence to this argument and present a vision
  in which easier decisions are delegated to computers, while the more
  difficult ones are handled by humans.

\end{abstract}

\begin{document}

\maketitle

\thispagestyle{firststyle}

\section*{Introduction}

Digital technology is causing a sea-change in all parts of the medical
profession. In particular the meteoric rise of AI in general and deep
learning in particular raises the possibility that doctors will be
replaced computers~\cite{Mukherjee2017}. The father of deep learning,
Geoff Hinton, said in 2017: "It's just completely obvious that that in
ten years deep learning is going to do better than Radiologists
... They should stop training radiologists now".

Other deep learning researchers provide a more nuanced
perspective. Sebastian
Thrun~\cite{Mukherjee2017,esteva2017dermatologist} argues that
"... deep learning devices will not replace dermatologists and
radiologists. They will {\em augment} professionals, offering the
expertise and assistance".

\Org{Artificial Intelligence and Intelligence Augmentation}{
Using computers to augment human intelligence rather replace it is
both tantalizing and mundane. On the heady side, consider
cyborgs whose anatomy is part human, part artificial and can with
equal ease solve complex equations or write poetry. On the mundane
side, think of smartphones that are quickly becoming an inseparable
part of our person.
 
The idea of using computers to augment or amplify human intelligence
has a very long history. The acronyms AI (Artificial intelligence) and
IA (Intelligence Amplification or Intelligence Augmentation) have both
become popular in the early
1960's\cite{ashby1957introduction,engelbart1962augmenting}. These
days, the acronym AI is popular, while the acronym IA is not. However,
Sebastian Thrun's statement indicates that the idea of Intelligence
augmentation is still on people's mind.}

{\bf What would IA look like when applied to medicine?} that is the
question we aim to answer here.  We argue that an important ingredient
of the answer is to introduce to AI agents a level of
humility. Specifically, to design classifiers, such as DNNs, to say "I
don't know".

\section*{Labels, ground truth and testing}

\ML{Supervised Learning and ground truth}{Roughly
  speaking, machine learning (ML) can be divided into {\em
    unsupervised} learning and {\em supervised} learning. In both, the
  task of the learning algorith is transform a set of {\em examples}
  into a {\em model}. In unsupervised learning the examples are
  undifferentiated raw measurements. In {\em supervised} learning,
  which is the focus of this article, each example consists of an {\em
    input} and a {\em label}. Typically, the labels are provided by a
  human expert. These labels define the {\em ground truth} and the
  goal of the learning algorithm is to make predictions that diverge
  as little as possible from the ground truth.}


\ML{Skin cancer diagnosis using Deep Neural Networks}{One of the
  papers that provided evidence that deep neural networks might be
  able to outperform humans is the work of Esteva et
  al~\cite{esteva2017dermatologist}. They trained a Deep neural
  network to classify images of skin into three categories: benign,
  malignant and non-cancerous. The network was then tested, along with
  twenty five dermatologists on images which were labeled by a
  pathologist analysis of the biopsy. The neural network outperformed
  the human dermatologist. This is, without a doubt, an impressive
  finding. However, it is based on a retrospective analysis, in other
  words, an analysis of historical data. To predict the performance of
  the DNN when used in a dermatology practice we need to how a
  dermatologist, or any other diagnosticians, arrives at their final
  diagnostics.}

In their famous work, Esteva et al. set out to show that a classifier
trained by machine learning can performs as well as or better than
expert dermatologists.  In this application of supervised learning
each example consists of an input image of a skin patch and an output
label that is ``benign'' or ``melignant''

As they wanted to compare the system to human dermatologists they
needed a better ground truth than that provided by the dermatologists.
To that end they used the diagnosis of a biopsy as ground truth. It is
arguable that this label is more accurate than the one given by the
dermatologist, even though it depends on the human judgement of the pathologist.

However, even if we assume that pathologists labels are more reliable
than dermatology labels, the requirement that each example corresponds
to a biopsy introduces a significant bias. Under normal circumstances, patients get
biopsied only if the dermatologist thinks there is a chance of
melignancy. Therefor, the set of biopsied examples is biased towards
melignancy. It is likely that using a classifier trained in this way
on an unfiltered stream of patients will increase the number of
patients unnecessarily getting a biopsy.

\section*{Uncertainty in medicine}

\Medicine{Patient Monitoring}{Take the patient monitor widely used in
  the intensive care unit (ICU), operation room (OR), or emergency
  room (ER) as an example. It is now common to analyze biosignals
  recorded from the patient monitor to train an intelligent
  \cite{Johnson2016} or alarm \cite{fleischman2019emergency}
  system. \yoav{Say a few words about what is an itelligent alarm
    system. Does it adapt to the patient. Is this related to alarm
    Fatigue?}
  However, it has been long debate if the recorded biosignals are suitable for this purpose, due to its ``blackbox'' nature \cite{Feldman2006,Shelley2016,Cannesson2016}. 
\yoav{Is the problem with the biosignals or with the system? Regarding black-boxes, as long as we allow the box to output I don't know when the prediction is unstable. Also, per-example explanations can be derived from the features that contribute the most from the score, assuming the learner is biased towards sparse classifiers classifier is a sparse one, such as boosting or Lasso.}
}

\Medicine{Blood pressure monitoring}{Recently, some delicate artifacts
  have been reported \cite{lin2019unexpected} regarding the pulse
  transit time that has been shown to reflect blood pressure
  information \cite{gesche2012continuous}. This problem should be
  viewed as a more complicated version of the ``heterogenous noise''
  issue commonly considered in the statistical literature.  \yoav{the
    way this is written, it sounds off topic. Is there an aspect of
    automation or ML that will connect it to the article?}  }

\Medicine{Inter-rater agreement}{in practice physicians need to make a
  decision when sitting on the ``gray area'' that is not covered by
  the protocol. Over this gray area, different physicians may make
  different decisions based on their experience or the information
  they have at hand. In some cases, physicians can achieve a reliable
  decision making, probably with sufficient clinical information
  \cite{mehta2011agreement} or if only the major information is needed
  \cite{atiya2003interobserver}.

  \yoav{Do we need to explain that sometimes inter-rater agreement is
    high? }}

  \Medicine{Identifying sleep stages}{ The American Academy of Sleep
    Medicine (AASM) publishes criteria for manual sleep stage
    annotation and
    sleep apnea detection. This annotation is based on manual analysis
    of biosignals\cite{Iber2007,berry2012aasm}. However, it is well known
    that the inter-rater agreement rate of sleep stage annotation
    among experienced experts is only about 76\% over normal subjects
    and about 71\% over subjects with sleep apnea
    \cite{norman2000interobserver}.
  }

  \Medicine{Low inter-rater agreement}{This low inter-rater agreement can
    be found in many clinical problems
    \cite{brosnan2015modest,venhola2003interobserver,moncada2011reading}.
    \yoav{Please elaborate.}}

    The issue underlying the inter-rater agreement is
  subtle. [physiological knowledge, phase transition, available
  information, treatment target, economic consideration]

\iffalse
  \Medicine{Inter-Rater agreement}{
  A direct consequence of this low inter-rater agreement is a
  questionable trained ``artificial intelligence''. It is possible
  that we magically obtain a dataset that contains information that is
  sufficient for the decision making, while the information is too
  subtle so that it is not considered in the protocol, and we also
  magically obtain labels from a magical master that can see though
  all the information and provide the correct decision. However, by
  doing a simple math, we shall not count on such a magic and should
  come back to the protocol itself.

  \yoav{ Can you describe a particular interesting / illuminating /
    convincing case?}
}
\fi


{\bf Sources of uncertainty in medical diagnosis.}
\begin{itemize}
  \item{\bf The diagnostic process of elimination}
  \item{\bf Data Quality, Calibration, resolution} Discuss issue as
    placement of sensors, lighting when analyzing skin lesions. Sensing back for re-testing.
  \end{itemize}

 {\bf Hiding Uncertainty}
  \begin{itemize}
    \item {\bf Psychological reasons} Both doctor and patient prefer
      the projection of certitude.
    \item {\bf Protocols}
    \item {\bf diagnostic devices} Secrecy of the internal code limits
      the trustworthiness of the alarms.
    \item{\bf Alarm Fatigue}
  \end{itemize}

  {\bf Quantifying Confidence} With experience comes confidence. In
  other words, diagnostic options that contradict the accumulated
  experience are eliminated.
  
  {\bf confidence through aggregation}
  \begin{itemize}
    \item A good way to increase diagnostic certainty is to solicit
      the experience of a diverse group of doctors.
    \item If there is a clear majority for one diagnostic outcome,
      then the overall confidence in the diagnostics is high.
    \item This certainty is very different from the the conditional.
      probability of the disease given the diagnostic. The first is
      akin to saying: 95\% of the dermatologists would give the same
      diagnostics. The second defines the probability that, if we had
      access to ground truth, then 95\% of the patients that recieve
      this diagnostics have the corresponding condition.
    \end{itemize}
    
\section*{Uncertainty in Machine Learning}

One can define ``confidence'' in machine learning. The definition follows a
similar logic to the one used for human diagnosticians in the previous
section. The yardstick by which we measure confidence of predicting a
label is ``how much do alternative labels contradict previous
experience?''.
More formally, we ask how much do we need to change the training data
so that it supports an alternative label.



\ML{Uncertainty vs. accurracy}{using ROC curves}

\begin{itemize}
  \item Bootstrap samples.
  \item Samples from different hospitals.
  \item Easy and hard cases.
  \end{itemize}

\section*{Agency and Augmentation}
\yoav{Doctors need to adapt. Why would doctors prefer to adapt than to
  resist? What is the migration path for augmentation in medicine?}
\begin{itemize}
\item{\bf Computer aided diagnostics}
  Especially with very large data: ecg for 14 says.... \\
  ~\\

Pathology.

\item {\bf Dissemination of expertise}
Computers, trained by experts, can help novices.  Serves a function
similar to score-cards.

Teaching young diagnostics
\item { \bf Confidence, Trust and adoption of technology}
\end{itemize}

\section*{Summary}

%\bibliographystyle{alpha} 
\bibliography{medbib}

\end{document}